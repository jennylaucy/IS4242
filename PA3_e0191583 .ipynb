{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "+ Rename this file to `<your nusnetid>.ipynb`. For example, `e0321286.ipynb`.\n",
    "+ The completed assignment must be uploaded to LumiNUS (Files > Assignment Submissions > PA3)\n",
    "+ Submission deadline: 11AM, 9th October\n",
    "+ Late Submission Rules: (1) 10 points will be deducted if submitted after 11 AM, 9th October and before 11 AM, 10th October; (2) Submissions after 11 AM, 10th October will not be graded and will be given **zero** marks.\n",
    "\n",
    "+ This assignment is of 20 marks total. There are two sections I and II.\n",
    "+ The  <font color='purple'> marks </font> associated with each question is mentioned at the end of the question in  <font color='purple'> purple </font> color.\n",
    "+ Questions with <font color='blue'> blue </font> color are not graded, but you are encouraged to answer them for your better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Understanding Grid Search and Pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1) Reimplement lines 2-7 of the code below by making all of the following modifications.** <font color='purple'> (2) </font> \n",
    "\n",
    "(a) Use sklearn's KFold() and find the best estimator using AUC scores obtained in each cross-validation step.\n",
    "\n",
    "(b) Do NOT use GridSearchCV(). You can use Pipeline(). \n",
    "\n",
    "Select the best combination of hyperparameters based on 3-fold cross-validation scores. \n",
    "\n",
    "Print the average AUC score for each cross-validation fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6555555555555556\n",
      "0.7777777777777777\n",
      "pca: 8 lr: 1\n"
     ]
    }
   ],
   "source": [
    "# #1\n",
    "# X,Y = make_classification(n_samples=100, n_features=20, n_informative=10, n_redundant=10, n_classes=2, n_clusters_per_class=1, shift=2,scale=None, random_state=0)\n",
    "# #2\n",
    "# parameters = dict(pca__n_components=[5,8],lr__C=[1, 3])\n",
    "# #3\n",
    "# estA = Pipeline(steps=[('pca', PCA()), ('lr', LogisticRegression())])\n",
    "# #4\n",
    "# estimator_A = GridSearchCV(estA, parameters, cv=3, scoring = 'roc_auc')\n",
    "# #5\n",
    "# estimator_A.fit(X,Y)\n",
    "# #6\n",
    "# A = estimator_A.best_estimator_\n",
    "# #7\n",
    "# print(estimator_A.best_params_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X,Y = make_classification(n_samples=100, n_features=20, n_informative=10, n_redundant=10, n_classes=2, n_clusters_per_class=1, shift=2,scale=None, random_state=0)\n",
    "parameters = dict(pca__n_components=[5,8],lr__C=[1, 3])\n",
    "estA = Pipeline(steps=[('pca', PCA()), ('lr', LogisticRegression())])\n",
    "\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "best = ''\n",
    "score = 0\n",
    "seed = 0\n",
    "kfold = KFold(n_splits=3, random_state=seed)\n",
    "\n",
    "for train_index, test_index in kfold.split(X, Y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "\n",
    "for pca in parameters[\"pca__n_components\"]:\n",
    "    for lr in parameters[\"lr__C\"]:\n",
    "        estA.set_params(pca__n_components = pca)\n",
    "        estA.set_params(lr__C = lr)\n",
    "        scores = cross_val_score(estA, X_test, Y_test, cv = 3, scoring=\"roc_auc\")\n",
    "        if (scores.mean() > score):\n",
    "            score = scores.mean()\n",
    "            best = 'pca: ' + str(pca) + \" \" + \"lr: \" + str(lr)\n",
    "    print(scores.mean())\n",
    "    \n",
    "    \n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2) Reimplement lines 2 and 3 of the code below by making all of the following modifications.** <font color='purple'> (2) </font> \n",
    "\n",
    "(a) Use fit() and transform() for each operation in the pipeline\n",
    "\n",
    "(b) Use KFold() and find the mean and standard deviation of AUC scores obtained over the 3 folds\n",
    "\n",
    "(c) Do NOT use the Pipeline() object or cross_val_score() function\n",
    "\n",
    "Ensure that there is no data leakage. \n",
    "\n",
    "Print the AUC score in each fold and the mean and standard deviation of AUC scores over the 3 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98269896 0.90441176 0.96323529] 0.9501153402537486 0.03327983578332014\n",
      "[0.9722222222222222, 0.8685185185185185, 0.9407407407407408] 0.9271604938271604 0.04341222462349938\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "X,Y = make_classification(n_samples=100, n_features=20, n_informative=10, n_redundant=10, n_classes=2, n_clusters_per_class=1, shift=2,scale=None, random_state=0)\n",
    "#2\n",
    "estA = Pipeline(steps=[('pca', PCA()), ('lr', LogisticRegression())])\n",
    "#3\n",
    "scores = cross_val_score(estA, X, Y, cv=3, scoring='roc_auc')\n",
    "#4\n",
    "print(scores, np.mean(scores), np.std(scores))\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, roc_curve, auc\n",
    "\n",
    "X,Y = make_classification(n_samples=100, n_features=20, n_informative=10, n_redundant=10, n_classes=2, n_clusters_per_class=1, shift=2,scale=None, random_state=0)\n",
    "kfold = KFold(n_splits=3)\n",
    "auc = []\n",
    "for train_index, test_index in kfold.split(X, Y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    pca = PCA()\n",
    "    lr = LogisticRegression()\n",
    "    pca.fit(X_train)\n",
    "    pca.transform(X_train)\n",
    "    lr.fit(X_train, Y_train)\n",
    "    #y_pred = lr.predict(X_test)  \n",
    "#     roc_auc = roc_auc_score(Y_test, y_pred)\n",
    "#     auc.append(roc_auc)\n",
    "\n",
    "    roc_auc=roc_auc_score(Y_test, lr.predict_proba(X_test)[:,1])\n",
    "    auc.append(roc_auc)\n",
    "\n",
    "\n",
    "        \n",
    "print(auc, np.mean(auc), np.std(auc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3) Evaluate the performance of the model from question 2 (trained on X,Y) on the test set (X2) generated below.**\n",
    "\n",
    "Use metrics precision, recall, sensitivity, specificity and AUC. <font color='purple'> (0.5) </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5463659147869674\n",
      "Recall: 0.8650793650793651\n",
      "Sensitivity :  0.2701612903225806\n",
      "Specificity :  0.8650793650793651\n",
      "AUC scores:  [0.97776822 0.96213425 0.95281649]\n"
     ]
    }
   ],
   "source": [
    "X2,Y2 = make_classification(n_samples=500, n_features=20, n_informative=10, n_redundant=10, n_classes=2, n_clusters_per_class=1, shift=2,scale=None, random_state=0)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y_pred = lr.predict(X2)\n",
    "print('Precision:', precision_score(Y2, Y_pred))\n",
    "print('Recall:', recall_score(Y2, Y_pred))\n",
    "#print('AUC:', roc_auc_score(Y2, y_pred))\n",
    "# false_positive_rate, true_positive_rate, thresholds = roc_curve(Y2, y_pred)\n",
    "# roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "# print('AUC:', roc_auc)\n",
    "\n",
    "\n",
    "cm1 = confusion_matrix(Y2, Y_pred)\n",
    "#print('Confusion Matrix : \\n', cm1)\n",
    "\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "\n",
    "Y_pred_prob = lr.predict_proba(X2)[:, 1]\n",
    "fpr1, tpr1, thresholds1 = roc_curve(Y2, Y_pred_prob)\n",
    "\n",
    "\n",
    "auc = cross_val_score(lr, X2, Y2, cv=3, scoring='roc_auc')\n",
    "print(\"AUC scores: \", auc)\n",
    "\n",
    "# #def plot_roc(fpr, tpr):\n",
    "# plt.figure()\n",
    "# roc_auc = auc(fpr1, tpr1)\n",
    "# plt.plot(fpr1, tpr1, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\n",
    "# plt.title('ROC'); plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()\n",
    "\n",
    "# #plot_roc(fpr1,tpr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Kaggle Competition\n",
    "In this section, we will attempt a relatively simple Kaggle competition: Porto Seguro's Safe Driver Prediction. The aim is to predict if a driver will file an insurance claim next year. Please read the description of the challenge on [Kaggle](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction). The competition is closed and we cannot participate in it. So, we'll just use the training data provided in this assignment.  \n",
    "\n",
    "Create a Kaggle account (if you don't have one) and download `train.csv` from the Data tab on the competition webpage. Read the Data description carefully. We'll build and evaluate end-to-end machine learning pipelines using sklearn functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4) Data Loading** <font color='purple'> (2) </font> \n",
    "\n",
    "(i) Load the data into a dataframe `df`, and print the number of observations (rows). This is a big dataset, so use the following code to sample only 1% of the observations using the code:`df = df.sample(frac=0.01, random_state=0)`. </font>  <font color='purple'> (0.5) </font> \n",
    "\n",
    "Use this sampled data for all the remaining questions in this section. **Make sure you set `random_state=0`, so your code can be reproduced exactly.** Then separate the target variable `Y` from the predictors `X`. Note that the `id` is not a predictor. <font color='blue'> Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "(ii) Print the number of classes in the target variable, and number of observations available in each class. <font color='purple'> (0.5) </font> \n",
    "    \n",
    "(iii) Split the data for training (80%) (`X_train, Y_train`) and testing (20%) (`X_test, Y_test`). Print the number of observations in the training set. <font color='purple'> (0.5) </font> \n",
    "    \n",
    "(iv) The features in this dataset are heterogeneous. Look at the column names and make separate lists of names for categorical (nominal) features `cat_feat`, binary features `bin_feat`, and numerical features (continuous and ordinal features) `num_feat`. Note that, in this analysis, we are considering binary features separately, not as part of categorical or numerical. Print the number of features in each of these 3 lists. <font color='purple'> (0.5) </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df = df.sample(frac=0.01, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5952,)\n",
      "number of classes in target variable  2\n",
      "number of observations:\n",
      " 0    5740\n",
      "1     212\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#print(len(df.index))\n",
    "#print(df.dtypes)\n",
    "Y = df['target']\n",
    "columns = ['target', 'id']\n",
    "X = df.drop(columns, axis = 1)\n",
    "print(Y.shape)\n",
    "print('number of classes in target variable ', len(Y.unique()))\n",
    "print('number of observations:\\n', Y.value_counts())\n",
    "\n",
    "\n",
    "#print(X)\n",
    "#print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of observation in training set 4761\n",
      "number of observation in training set 4761\n"
     ]
    }
   ],
   "source": [
    "testsize = 0.2  # We are choosing 20% of the data to be test data\n",
    "seed = 0\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = testsize, random_state = seed)\n",
    "print('number of observation in training set', len(X_train))\n",
    "print('number of observation in training set',len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat feat:  14\n",
      "bin feat:  17\n",
      "num feat:  26\n"
     ]
    }
   ],
   "source": [
    "cat_feat = []\n",
    "bin_feat = []\n",
    "num_feat = []\n",
    "\n",
    "for col in X:\n",
    "    if ('cat' in col):\n",
    "        cat_feat.append(col)\n",
    "    elif ('bin' in col):\n",
    "        bin_feat.append(col)\n",
    "    else:\n",
    "        num_feat.append(col)\n",
    "\n",
    "# print(cat_feat)\n",
    "# print(bin_feat)\n",
    "# print(num_feat)\n",
    "\n",
    "#print(df.shape)\n",
    "print(\"cat feat: \", len(cat_feat))\n",
    "print(\"bin feat: \", len(bin_feat))\n",
    "print(\"num feat: \", len(num_feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5) Preprocessing** <font color='purple'> (3) </font>  \n",
    "\n",
    "Create a `preprocessor` pipeline with the following transformation steps.\n",
    "\n",
    "(i) \"Appropriate\" *Imputers* for different types of data (categorical, binary and numerical). <font color='purple'> (1) </font> \n",
    "\n",
    "(ii) A *OneHotEncoder* for the categorical features to convert their integer representations to one-hot-vectors. <font color='purple'> (1) </font> \n",
    "\n",
    "(iii) A *Scaler* for standardising the resulting data from the above two steps. <font color='purple'> (0.5) </font>\n",
    "\n",
    "(iv) Print the total number of features after transforming `X_train` with the above constructed `preprocessor`. <font color='purple'> (0.5) </font>\n",
    "\n",
    "\n",
    "Hint: Use `ColumnTransformer` to consolidate first two steps.\n",
    "\n",
    "Note: Only the `preprocessor` Pipeline object should be used to `fit` and `transform` the training data. The constituent objects (Imputer, OneHotEncoder and Scaler) should not be separately used to `fit` and `transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# X[cat_feat] = X[cat_feat].apply(lambda col: le.fit_transform(col))\n",
    "# index = [df.columns.get_loc(c) for c in cat_feat if c in X]\n",
    "#print(index)\n",
    "#print(df.dtypes)\n",
    "\n",
    "\n",
    "categorical_features = cat_feat\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('oneshot',OneHotEncoder(sparse=False, categorical_features = 'all', categories='auto', handle_unknown='ignore'))])\n",
    "\n",
    "\n",
    "bin_features = bin_feat\n",
    "bin_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent'))])\n",
    "\n",
    "\n",
    "numeric_features = num_feat\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))])\n",
    "\n",
    "\n",
    "colTransformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('bin', bin_transformer, bin_features)])\n",
    "\n",
    "\n",
    "preprocessor = Pipeline(steps=[\n",
    "    ('imputer', colTransformer), \n",
    "    ('f_scaler', StandardScaler())])\n",
    "\n",
    "\n",
    "\n",
    "preprocessor.fit(X_train, Y_train)\n",
    "preprocessor.transform(X_train)\n",
    "print(len(X_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6) Building Different Estimators** <font color='purple'> (6) </font> \n",
    "\n",
    "Build four different estimators using the `preprocessor` object from the previous question and the following pipelines of operations:\n",
    "\n",
    "i. `est1`: Preprocessing -> Classification <font color='purple'> (1.5) </font> \n",
    "\n",
    "ii. `est2`: Preprocessing -> Feature Selection -> Classification <font color='purple'> (1.5) </font> \n",
    "\n",
    "iii. `est3`: Preprocessing -> Feature Selection -> PCA -> Classification <font color='purple'> (1.5) </font> \n",
    "\n",
    "iv. `est4`: Preprocessing -> PCA -> Classification <font color='purple'> (1.5) </font> \n",
    "\n",
    "The four estimators should differ in one or more functions chosen for feature selection and/or classification (e.g. you could use [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) and LogisticRegression in `est2`, and [VarianceThreshold](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold) and LogisticRegression in `est3`. \n",
    "\n",
    "You are free to use any classifier or [feature selection method](https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection) from sklearn (not just the ones listed above or discussed in class) that can be used in these pipelines.\n",
    "\n",
    "Identify at least one hyperparameter per operation in each pipeline. \n",
    "\n",
    "For each selected hyperparameter, choose two values -- they may or may not be the same across pipelines. \n",
    "\n",
    "Use grid search over the hyperparameter values with 5-fold cross validation and `roc_auc` scoring. \n",
    "\n",
    "Evaluate the four best estimators over 5-fold cross validation on the training data. \n",
    "\n",
    "Print the mean and std of AUC over the 5-folds for each best estimator. \n",
    "\n",
    "<font color='blue'> Among the 4 best estimators found above, which has highest AUC score? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.552 mean  +/-  0.039 std\n",
      "0.5516044080706588 0.03875715215749445\n",
      "AUC = 0.599 mean  +/-  0.054 std\n",
      "0.5990014894216337 0.05436998871142327\n",
      "AUC = 0.574 mean  +/-  0.024 std\n",
      "0.5740572462677641 0.02372507251869663\n",
      "AUC = 0.574 mean  +/-  0.024 std\n",
      "0.5743066086332435 0.023708982241985786\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "base = 0\n",
    "est1 = Pipeline(steps=[\n",
    "    #('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(solver='lbfgs', \n",
    "                                      multi_class='auto'))])\n",
    "hparam_grid = {'classifier__C': [1., 0.5]}\n",
    "estimator1 = GridSearchCV(est1, hparam_grid, cv=5, scoring = 'roc_auc')\n",
    "estimator1.fit(X_train, Y_train)\n",
    "scores = cross_val_score(estimator1.best_estimator_, X, Y, cv=5, scoring='roc_auc')\n",
    "print('AUC = {:.3f} mean  +/-  {:.3f} std'.format(scores.mean(), scores.std()))\n",
    "print(scores.mean(), scores.std())\n",
    "\n",
    "if (scores.mean() >= base):\n",
    "    M = estimator1\n",
    "    base = scores.mean()\n",
    "    \n",
    "\n",
    "est2 = Pipeline(steps=[\n",
    "    #('preprocessor', preprocessor),\n",
    "    ('selectKBest', SelectKBest(k=10)),\n",
    "    ('classifier', LogisticRegression(solver='lbfgs', \n",
    "                                      multi_class='auto'))])\n",
    "hparam_grid = {'selectKBest__k': [5, 10],\n",
    "               'classifier__C': [1., 0.5]}\n",
    "estimator2 = GridSearchCV(est2, hparam_grid, cv=5, scoring = 'roc_auc')\n",
    "estimator2.fit(X_train, Y_train)\n",
    "scores = cross_val_score(estimator2.best_estimator_, X, Y, cv=5, scoring='roc_auc')\n",
    "print('AUC = {:.3f} mean  +/-  {:.3f} std'.format(scores.mean(), scores.std()))\n",
    "print(scores.mean(), scores.std())\n",
    "\n",
    "if (scores.mean() >= base):\n",
    "    M = estimator2 \n",
    "    base = scores.mean()\n",
    "\n",
    "\n",
    "est3 = Pipeline(steps=[\n",
    "    #('preprocessor', preprocessor),\n",
    "    ('varianceThreshold', VarianceThreshold(threshold=1)),\n",
    "    ('dim_reducer', PCA(n_components=4)),\n",
    "    ('classifier', LogisticRegression(solver='lbfgs', \n",
    "                                      multi_class='auto'))])\n",
    "hparam_grid = {'varianceThreshold__threshold': [0.1, 0.2],\n",
    "               'dim_reducer__n_components': [5, 10],\n",
    "               'classifier__C': [1., 0.5]}\n",
    "estimator3 = GridSearchCV(est3, hparam_grid, cv=5, scoring = 'roc_auc')\n",
    "estimator3.fit(X_train, Y_train)\n",
    "scores = cross_val_score(estimator3.best_estimator_, X, Y, cv=5, scoring='roc_auc')\n",
    "print('AUC = {:.3f} mean  +/-  {:.3f} std'.format(scores.mean(), scores.std()))\n",
    "print(scores.mean(), scores.std())\n",
    "\n",
    "if (scores.mean() >= base):\n",
    "    M = estimator3 \n",
    "    base = scores.mean()\n",
    "\n",
    "    \n",
    "est4 = Pipeline(steps=[\n",
    "    #('preprocessor', preprocessor),\n",
    "    ('dim_reducer', PCA(n_components=4)),\n",
    "    ('classifier', LogisticRegression(solver='lbfgs', \n",
    "                                      multi_class='auto'))])\n",
    "hparam_grid4 = {'dim_reducer__n_components': [5, 10],\n",
    "               'classifier__C': [1., 0.5]}\n",
    "estimator4 = GridSearchCV(est4, hparam_grid4, cv=5, scoring = 'roc_auc')\n",
    "estimator4.fit(X_train, Y_train)\n",
    "scores = cross_val_score(estimator4.best_estimator_, X, Y, cv=5, scoring='roc_auc')\n",
    "print('AUC = {:.3f} mean  +/-  {:.3f} std'.format(scores.mean(), scores.std()))\n",
    "print(scores.mean(), scores.std())\n",
    "\n",
    "\n",
    "if (scores.mean() >= base):\n",
    "    M = estimator4\n",
    "    base = scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7) Class Imbalance** <font color='purple'> (3.5) </font> \n",
    "\n",
    "Learn what is class imbalance by reading [this article](https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28). \n",
    "\n",
    "Additional links to packages for handling class imbalance in python: [Imbalanced-learn.org](http://imbalanced-learn.org/en/stable/index.html), [Over-sampling](http://imbalanced-learn.org/en/stable/over_sampling.html#a-practical-guide), [Under-sampling](http://imbalanced-learn.org/en/stable/under_sampling.html).\n",
    "\n",
    "Install the `imbalanced-learn` package within Anaconda (See installation details in the link given above). We'll denote the estimator with the best average AUC in the previous question by `M`. \n",
    "\n",
    "+ Print the `accuracy_score`, `precision_score`, `recall_score` and `confusion_matrix` of `M` on the test dataset. <font color='purple'> (0.5) </font>  \n",
    "<font color='blue'> In this case, dose high accuracy score reflect better performance? </font> \n",
    "\n",
    "+ Create two more estimators\n",
    "    1. `M_over`\n",
    "    2. `M_under`  \n",
    "    \n",
    "  by adding \n",
    "  \n",
    "    1. SMOTE \n",
    "    2. under-sampler\n",
    "    \n",
    "    respectively to the preprocessing pipeline (just before scaling transformation) of M.\n",
    "    \n",
    "    Note that for this you may have to use the [Pipeline object](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.pipeline.Pipeline.html) in imbalanced-learn.\n",
    "\n",
    "    Train both these estimators on the training data. \n",
    "\n",
    "    Print the mean and standard deviation of the AUC values, over the 5 folds, for each estimator on the training data.\n",
    "\n",
    "    Print the accuracy_score, precision, recall and confusion_matrix of `M_over` and `M_under` on the test data. <font color='purple'> (3) </font>  \n",
    "\n",
    "<font color='blue'> How does SMOTE or under-sampling impact (increase/decrease) the learning performance on training and test data as compared to `M`? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy = 96.31%\n",
      "test precision = 0.00%\n",
      "test recall = 0.00%\n",
      "confusion matrix:\n",
      "  [[1147    0]\n",
      " [  44    0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "Y_pred = M.predict(X_test)\n",
    "print(\"test accuracy = {:.2%}\".format(accuracy_score(Y_test, Y_pred)))\n",
    "print(\"test precision = {:.2%}\".format(precision_score(Y_test, Y_pred)))\n",
    "print(\"test recall = {:.2%}\".format(recall_score(Y_test, Y_pred)))\n",
    "\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "print(\"confusion matrix:\\n \", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# smt = SMOTE()\n",
    "\n",
    "# preprocessor1 = Pipeline(steps=[\n",
    "#     ('imputer', colTransformer), \n",
    "#     ('smt', smt),\n",
    "#     ('f_scaler', StandardScaler())])\n",
    "\n",
    "# preprocessor2 = Pipeline(steps=[\n",
    "#     ('imputer', colTransformer),\n",
    "#     ('randomUnderSampling', RandomUnderSampler()),\n",
    "#     ('f_scaler', StandardScaler())])\n",
    "\n",
    "#X_resampled, Y_resampled = smt.fit_resample(X_train, Y_train)\n",
    "# X_resampled, Y_resampled = smt.fit_resample(X, Y)\n",
    "# preprocessor1.fit(X_train, Y_train)\n",
    "# preprocessor1.transform(X_train)\n",
    "\n",
    "# est4 = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('dim_reducer', PCA(n_components=4)),\n",
    "#     ('classifier', LogisticRegression(solver='lbfgs', \n",
    "#                                       multi_class='auto'))])\n",
    "# hparam_grid4 = {'dim_reducer__n_components': [5, 10],\n",
    "#                'classifier__C': [1., 0.5]}\n",
    "# estimator4 = GridSearchCV(est4, hparam_grid4, cv=5, scoring = 'roc_auc')\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = testsize, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.559 mean  +/-  0.045 std\n",
      "0.5590136054421768 0.045314250968312014\n"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import make_pipeline\n",
    "#from sklearn.pipeline import Pipeline\n",
    "\n",
    "#print(df.dtypes)\n",
    "# est4 = Pipeline(steps=[\n",
    "#     ('preprocessor1', preprocessor1),\n",
    "#     ('dim_reducer', PCA(n_components=4)),\n",
    "#     ('classifier', LogisticRegression(solver='lbfgs', \n",
    "#                                       multi_class='auto'))])\n",
    "# hparam_grid4 = {'dim_reducer__n_components': [5, 10],\n",
    "#                'classifier__C': [1., 0.5]}\n",
    "\n",
    "\n",
    "\n",
    "M_over = Pipeline(steps=[\n",
    "    ('smt', SMOTE()),\n",
    "    ('est', est2)])\n",
    "\n",
    "# hparam_grid5 = {'dim_reducer__n_components': [5, 10],\n",
    "#                'classifier__C': [1., 0.5]}\n",
    "\n",
    "# #print(X)\n",
    "# M_over = GridSearchCV(est5, hparam_grid5, cv=5, scoring = 'roc_auc')\n",
    "# a = pd.DataFrame(X_train, columns=X.columns)\n",
    "# b = pd.DataFrame(Y_train, columns=['target'])\n",
    "\n",
    "# print(Y_train)\n",
    "# print(b)\n",
    "#M_over.fit(a, b)\n",
    "M_over.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "scores = cross_val_score(M_over, X, Y, cv=5, scoring='roc_auc')\n",
    "print('AUC = {:.3f} mean  +/-  {:.3f} std'.format(scores.mean(), scores.std()))\n",
    "print(scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.541 mean  +/-  0.027 std\n",
      "0.5410282604249869 0.0266091520822533\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "#print(df.dtypes)\n",
    "# est5 = Pipeline(steps=[\n",
    "#     ('preprocessor2', preprocessor2),\n",
    "#     ('dim_reducer', PCA(n_components=4)),\n",
    "#     ('classifier', LogisticRegression(solver='lbfgs', \n",
    "#                                       multi_class='auto'))])\n",
    "# hparam_grid5 = {'dim_reducer__n_components': [5, 10],\n",
    "#                'classifier__C': [1., 0.5]}\n",
    "\n",
    "# M_under = GridSearchCV(est5, hparam_grid5, cv=5, scoring = 'roc_auc')\n",
    "# a = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "# b = pd.DataFrame(Y_resampled, columns=['target'])\n",
    "# M_under.fit(a, b)\n",
    "\n",
    "M_under = Pipeline(steps=[\n",
    "    ('under', RandomUnderSampler()),\n",
    "    ('est4', est2)])\n",
    "M_under.fit(X_train, Y_train)\n",
    "scores = cross_val_score(M_under, X, Y, cv=5, scoring='roc_auc')\n",
    "print('AUC = {:.3f} mean  +/-  {:.3f} std'.format(scores.mean(), scores.std()))\n",
    "print(scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8) Ensemble Method**  <font color='purple'> (1) </font> \n",
    "\n",
    "To answer this question, learn about Random Forest Classifier. You may follow these links:\n",
    "[Understanding Random Forest](https://towardsdatascience.com/understanding-random-forest-58381e0602d2), and [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). \n",
    "\n",
    "+ Create a pipeline with the `preprocessor` object of Q5 and a RandomForest classifier having 100 estimators and a maximum depth of 3. Train it on the training set. \n",
    "\n",
    "    Print the `accuracy_sore`, precision, recall and `confusion_matrix` of this model on the test data. <font color='purple'> (1) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[1147    0]\n",
      " [  44    0]]\n",
      "Accuracy = 96.31%\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = testsize, random_state = 0)\n",
    "\n",
    "\n",
    "est1 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0))])\n",
    "\n",
    "\n",
    "est1.fit(X_train, Y_train)\n",
    "Y_pred = est1.predict(X_test)\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(Y_test, Y_pred))\n",
    "print(\"Accuracy = {:.2%}\".format(accuracy_score(Y_test, Y_pred)))\n",
    "print('Precision:', precision_score(Y_test, Y_pred))\n",
    "print('Recall:', recall_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
